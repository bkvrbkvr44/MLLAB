{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer weights and biases\n",
    "w = [[0.15, 0.2], [0.25, 0.3]]\n",
    "b = [0.35, 0.35]\n",
    "\n",
    "# Hidden layer weights and biases\n",
    "wh = [[0.4, 0.45], [0.5, 0.55]]\n",
    "bh = [0.6, 0.6]\n",
    "\n",
    "# Output layer weights and biases\n",
    "wo = [[0.01, 0.99], [0.01, 0.99]]\n",
    "bo = [0.01, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs):\n",
    "    # Calculate the inputs to the hidden layer\n",
    "    h_input = [sum([inputs[j] * w[i][j] for j in range(len(inputs))]) + b[i] for i in range(len(b))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the hidden layer\n",
    "    h_output = [sigmoid(x) for x in h_input]\n",
    "\n",
    "    # Calculate the inputs to the output layer\n",
    "    o_input = [sum([h_output[j] * wh[i][j] for j in range(len(h_output))]) + bh[i] for i in range(len(bh))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the output layer\n",
    "    o_output = [sigmoid(x) for x in o_input]\n",
    "\n",
    "    return h_output, o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, h_output, o_output, target):\n",
    "    # Calculate the error in the output layer\n",
    "    o_error = [target[i] - o_output[i] for i in range(len(target))]\n",
    "\n",
    "    # Calculate the delta for the output layer\n",
    "    o_delta = [o_error[i] * sigmoid_derivative(o_output[i]) for i in range(len(o_error))]\n",
    "\n",
    "    # Calculate the error in the hidden layer\n",
    "    h_error = [sum([o_delta[j] * wh[j][i] for j in range(len(o_delta))]) for i in range(len(h_output))]\n",
    "\n",
    "    # Calculate the delta for the hidden layer\n",
    "    h_delta = [h_error[i] * sigmoid_derivative(h_output[i]) for i in range(len(h_error))]\n",
    "\n",
    "    return o_delta, h_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(inputs, h_output, o_delta, h_delta, lr):\n",
    "    global w, wh, b, bh, wo, bo\n",
    "\n",
    "    # Update weights and biases in the output layer\n",
    "    wo = [[wo[i][j] + lr * o_delta[i] * h_output[j] for j in range(len(wo[0]))] for i in range(len(wo))]\n",
    "    bo = [bo[i] + lr * o_delta[i] for i in range(len(bo))]\n",
    "\n",
    "    # Update weights and biases in the hidden layer\n",
    "    wh = [[wh[i][j] + lr * h_delta[i] * h_output[j] for j in range(len(wh[0]))] for i in range(len(wh))]\n",
    "    bh = [bh[i] + lr * h_delta[i] for i in range(len(bh))]\n",
    "\n",
    "    # Update weights and biases in the input layer\n",
    "    w = [[w[i][j] + lr * inputs[j] * h_delta[i] for j in range(len(w[0]))] for i in range(len(w))]\n",
    "    b = [b[i] + lr * h_delta[i] for i in range(len(b))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, target, lr):\n",
    "    h_output, o_output = forward_pass(inputs)\n",
    "    o_delta, h_delta = backward_pass(inputs, h_output, o_output, target)\n",
    "    update_weights(inputs, h_output, o_delta, h_delta, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 100\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 2: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 3: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 4: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 5: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 6: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 7: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 8: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 9: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 10: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 11: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 12: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 13: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 14: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 15: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 16: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 17: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 18: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 19: Predicted Output = [0.5386244805683293, 0.4945947219146775], Loss = 0.26243511550546583\n",
      "Epoch 20: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 21: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 22: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 23: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 24: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 25: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 26: Predicted Output = [0.5386244805683292, 0.4945947219146775], Loss = 0.2624351155054658\n",
      "Epoch 27: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 28: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 29: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 30: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 31: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 32: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 33: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 34: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 35: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 36: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 37: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 38: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 39: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 40: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 41: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 42: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 43: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 44: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 45: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 46: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 47: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 48: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 49: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 50: Predicted Output = [0.5386244805683292, 0.4945947219146776], Loss = 0.2624351155054657\n",
      "Epoch 51: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 52: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 53: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 54: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 55: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 56: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 57: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 58: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 59: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 60: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 61: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 62: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 63: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 64: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 65: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 66: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 67: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 68: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 69: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 70: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 71: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 72: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 73: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 74: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 75: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 76: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 77: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 78: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 79: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 80: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 81: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 82: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 83: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 84: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 85: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 86: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 87: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 88: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 89: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 90: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 91: Predicted Output = [0.5386244805683292, 0.4945947219146777], Loss = 0.2624351155054657\n",
      "Epoch 92: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 93: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 94: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 95: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 96: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 97: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 98: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 99: Predicted Output = [0.5386244805683291, 0.4945947219146777], Loss = 0.26243511550546567\n",
      "Epoch 100: Predicted Output = [0.5386244805683291, 0.4945947219146778], Loss = 0.2624351155054656\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code)\n",
    "\n",
    "# Training Loop:\n",
    "for epoch in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        h_output, o_output = forward_pass(inputs)\n",
    "        print(f\"Epoch {epoch+1}: Predicted Output = {o_output}, Loss = {sum([(target[i] - o_output[i])**2 for i in range(len(target))])/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Error after first round of Backpropagation: 0.2624351155054656\n"
     ]
    }
   ],
   "source": [
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 1\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "\n",
    "h_output, o_output = forward_pass(inputs)\n",
    "error = sum([(target[i] - o_output[i])**2 for i in range(len(target))]) / 2\n",
    "print(f\"Total Error after first round of Backpropagation: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1 if x > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - math.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs):\n",
    "    # Calculate the inputs to the hidden layer\n",
    "    h_input = [sum([inputs[j] * w[i][j] for j in range(len(inputs))]) + b[i] for i in range(len(b))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the hidden layer\n",
    "    h_output = [relu(x) for x in h_input]\n",
    "\n",
    "    # Calculate the inputs to the output layer\n",
    "    o_input = [sum([h_output[j] * wh[i][j] for j in range(len(h_output))]) + bh[i] for i in range(len(bh))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the output layer\n",
    "    o_output = [relu(x) for x in o_input]\n",
    "\n",
    "    return h_output, o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Predicted Output = [0.44173849016736644, 0.47942862929534336], Loss = 0.22354062423761453\n",
      "Epoch 2: Predicted Output = [0.44176885387108966, 0.47924576826464294], Loss = 0.2236471142043646\n",
      "Epoch 3: Predicted Output = [0.4417987361769198, 0.47906341427663757], Loss = 0.22375317159731603\n",
      "Epoch 4: Predicted Output = [0.44182815976381157, 0.47888160269548413], Loss = 0.22385878781406846\n",
      "Epoch 5: Predicted Output = [0.441857145440291, 0.4787003662710294], Loss = 0.22396395475960806\n",
      "Epoch 6: Predicted Output = [0.44188571230583445, 0.4785197353471322], Loss = 0.22406866481164284\n",
      "Epoch 7: Predicted Output = [0.4419138778984022, 0.47833973805228946], Loss = 0.22417291078881782\n",
      "Epoch 8: Predicted Output = [0.4419416583293137, 0.47816040047408853], Loss = 0.22427668592156152\n",
      "Epoch 9: Predicted Output = [0.44196906840655487, 0.47798174681886973], Loss = 0.22437998382534147\n",
      "Epoch 10: Predicted Output = [0.44199612174750647, 0.4778037995578625], Loss = 0.22448279847612432\n",
      "Epoch 11: Predicted Output = [0.44202283088200633, 0.47762657956095816], Loss = 0.22458512418785287\n",
      "Epoch 12: Predicted Output = [0.4420492073465686, 0.477450106219174], Loss = 0.2246869555917671\n",
      "Epoch 13: Predicted Output = [0.44207526177052453, 0.47727439755677925], Loss = 0.22478828761741548\n",
      "Epoch 14: Predicted Output = [0.442101003954771, 0.47709947033396555], Loss = 0.22488911547520984\n",
      "Epoch 15: Predicted Output = [0.4421264429437648, 0.4769253401408724], Loss = 0.22498943464039514\n",
      "Epoch 16: Predicted Output = [0.4421515870913388, 0.4767520214837068], Loss = 0.2250892408383121\n",
      "Epoch 17: Predicted Output = [0.44217644412086915, 0.4765795278636345], Loss = 0.22518853003084363\n",
      "Epoch 18: Predicted Output = [0.4422010211802782, 0.4764078718490591], Loss = 0.2252872984039439\n",
      "Epoch 19: Predicted Output = [0.44222532489231164, 0.4762370651418572], Loss = 0.22538554235615826\n",
      "Epoch 20: Predicted Output = [0.4422493614004992, 0.47606711863808515], Loss = 0.22548325848804968\n",
      "Epoch 21: Predicted Output = [0.44227313641116256, 0.4758980424836303], Loss = 0.22558044359245336\n",
      "Epoch 22: Predicted Output = [0.44229665523181094, 0.4757298461252407], Loss = 0.2256770946454899\n",
      "Epoch 23: Predicted Output = [0.44231992280623367, 0.47556253835732676], Loss = 0.22577320879827237\n",
      "Epoch 24: Predicted Output = [0.4423429437465695, 0.4753961273648992], Loss = 0.2258687833692462\n",
      "Epoch 25: Predicted Output = [0.44236572236261223, 0.4752306207629709], Loss = 0.2259638158371099\n",
      "Epoch 26: Predicted Output = [0.4423882626885888, 0.4750660256327279], Loss = 0.22605830383426523\n",
      "Epoch 27: Predicted Output = [0.4424105685076239, 0.4749023485547451], Loss = 0.22615224514075188\n",
      "Epoch 28: Predicted Output = [0.4424326433740902, 0.47473959563950086], Loss = 0.22624563767862407\n",
      "Epoch 29: Predicted Output = [0.4424544906340239, 0.47457777255541855], Loss = 0.22633847950673344\n",
      "Epoch 30: Predicted Output = [0.44247611344376936, 0.4744168845546529], Loss = 0.22643076881587906\n",
      "Epoch 31: Predicted Output = [0.44249751478700633, 0.47425693649680745], Loss = 0.22652250392429743\n",
      "Epoch 32: Predicted Output = [0.4425186974902945, 0.474097932870767], Loss = 0.2266136832734582\n",
      "Epoch 33: Predicted Output = [0.44253966423726465, 0.47393987781480185], Loss = 0.22670430542414366\n",
      "Epoch 34: Predicted Output = [0.44256041758157055, 0.47378277513509487], Loss = 0.22679436905278333\n",
      "Epoch 35: Predicted Output = [0.4425809599587058, 0.4736266283228267], Loss = 0.22688387294802376\n",
      "Epoch 36: Predicted Output = [0.44260129369678464, 0.4734714405699405], Loss = 0.2269728160075121\n",
      "Epoch 37: Predicted Output = [0.4426214210263745, 0.4733172147837043], Loss = 0.2270611972348741\n",
      "Epoch 38: Predicted Output = [0.44264134408945915, 0.47316395360016816], Loss = 0.22714901573687146\n",
      "Epoch 39: Predicted Output = [0.44266106494760976, 0.47301165939661777], Loss = 0.22723627072071922\n",
      "Epoch 40: Predicted Output = [0.4426805855894287, 0.4728603343031053], Loss = 0.22732296149155348\n",
      "Epoch 41: Predicted Output = [0.4426999079373294, 0.4727099802131421], Loss = 0.2274090874500306\n",
      "Epoch 42: Predicted Output = [0.44271903385370853, 0.472560598793619], Loss = 0.22749464809005254\n",
      "Epoch 43: Predicted Output = [0.4427379651465636, 0.4724121914940282], Loss = 0.2275796429966015\n",
      "Epoch 44: Predicted Output = [0.44275670357459973, 0.47226475955504127], Loss = 0.2276640718436766\n",
      "Epoch 45: Predicted Output = [0.4427752508518753, 0.47211830401650134], Loss = 0.22774793439232424\n",
      "Epoch 46: Predicted Output = [0.44279360865201944, 0.4719728257248799], Loss = 0.22783123048875153\n",
      "Epoch 47: Predicted Output = [0.44281177861206233, 0.4718283253402435], Loss = 0.22791396006251666\n",
      "Epoch 48: Predicted Output = [0.44282976233591054, 0.47168480334277363], Loss = 0.22799612312479003\n",
      "Epoch 49: Predicted Output = [0.442847561397497, 0.47154226003887834], Loss = 0.22807771976667698\n",
      "Epoch 50: Predicted Output = [0.442865177343633, 0.47140069556693015], Loss = 0.2281587501575994\n",
      "Epoch 51: Predicted Output = [0.44288261169659177, 0.4712601099026622], Loss = 0.22823921454373014\n",
      "Epoch 52: Predicted Output = [0.442899865956441, 0.4711205028642548], Loss = 0.2283191132464742\n",
      "Epoch 53: Predicted Output = [0.44291694160315065, 0.4709818741171353], Loss = 0.22839844666099346\n",
      "Epoch 54: Predicted Output = [0.44293384009849335, 0.47084422317851876], Loss = 0.2284772152547717\n",
      "Epoch 55: Predicted Output = [0.44295056288775586, 0.4707075494217103], Loss = 0.22855541956621503\n",
      "Epoch 56: Predicted Output = [0.4429671114012779, 0.4705718520801907], Loss = 0.22863306020328494\n",
      "Epoch 57: Predicted Output = [0.44298348705583346, 0.4704371302515032], Loss = 0.22871013784216132\n",
      "Epoch 58: Predicted Output = [0.4429996912558686, 0.47030338290095797], Loss = 0.22878665322593292\n",
      "Epoch 59: Predicted Output = [0.4430157253946084, 0.47017060886517154], Loss = 0.22886260716331264\n",
      "Epoch 60: Predicted Output = [0.4430315908550437, 0.4700388068554536], Loss = 0.22893800052737517\n",
      "Epoch 61: Predicted Output = [0.44304728901080975, 0.4699079754610541], Loss = 0.22901283425431562\n",
      "Epoch 62: Predicted Output = [0.44306282122696455, 0.4697781131522846], Loss = 0.22908710934222748\n",
      "Epoch 63: Predicted Output = [0.4430781888606782, 0.4696492182835216], Loss = 0.2291608268498977\n",
      "Epoch 64: Predicted Output = [0.44309339326183855, 0.4695212890961025], Loss = 0.22923398789561825\n",
      "Epoch 65: Predicted Output = [0.4431084357735834, 0.4693943237211259], Loss = 0.22930659365601203\n",
      "Epoch 66: Predicted Output = [0.4431233177327639, 0.4692683201821583], Loss = 0.22937864536487396\n",
      "Epoch 67: Predicted Output = [0.4431380404703462, 0.4691432763978613], Loss = 0.22945014431202296\n",
      "Epoch 68: Predicted Output = [0.4431526053117577, 0.4690191901845409], Loss = 0.22952109184216746\n",
      "Epoch 69: Predicted Output = [0.4431670135771816, 0.46889605925862643], Loss = 0.2295914893537816\n",
      "Epoch 70: Predicted Output = [0.4431812665818065, 0.4687738812390865], Loss = 0.229661338297992\n",
      "Epoch 71: Predicted Output = [0.4431953656360336, 0.4686526536497838], Loss = 0.22973064017747458\n",
      "Epoch 72: Predicted Output = [0.4432093120456465, 0.4685323739217746], Loss = 0.22979939654536108\n",
      "Epoch 73: Predicted Output = [0.44322310711194723, 0.46841303939555884], Loss = 0.22986760900415432\n",
      "Epoch 74: Predicted Output = [0.44323675213186176, 0.46829464732328085], Loss = 0.22993527920465207\n",
      "Epoch 75: Predicted Output = [0.44325024839801763, 0.46817719487088644], Loss = 0.23000240884488043\n",
      "Epoch 76: Predicted Output = [0.4432635971987976, 0.46806067912023935], Loss = 0.23006899966903388\n",
      "Epoch 77: Predicted Output = [0.44327679981837037, 0.4679450970711989], Loss = 0.23013505346642404\n",
      "Epoch 78: Predicted Output = [0.44328985753670225, 0.467830445643661], Loss = 0.23020057207043668\n",
      "Epoch 79: Predicted Output = [0.4433027716295514, 0.4677167216795662], Loss = 0.2302655573574955\n",
      "Epoch 80: Predicted Output = [0.4433155433684453, 0.4676039219448752], Loss = 0.2303300112460335\n",
      "Epoch 81: Predicted Output = [0.443328174020646, 0.4674920431315139], Loss = 0.23039393569547345\n",
      "Epoch 82: Predicted Output = [0.4433406648491022, 0.46738108185929045], Loss = 0.23045733270521374\n",
      "Epoch 83: Predicted Output = [0.4433530171123906, 0.46727103467778514], Loss = 0.23052020431362263\n",
      "Epoch 84: Predicted Output = [0.4433652320646488, 0.46716189806821257], Loss = 0.23058255259704047\n",
      "Epoch 85: Predicted Output = [0.4433773109554995, 0.4670536684452618], Loss = 0.23064437966878895\n",
      "Epoch 86: Predicted Output = [0.44338925502996795, 0.4669463421589097], Loss = 0.23070568767818747\n",
      "Epoch 87: Predicted Output = [0.4434010655283934, 0.46683991549621284], Loss = 0.23076647880957823\n",
      "Epoch 88: Predicted Output = [0.44341274368633643, 0.46673438468307676], Loss = 0.23082675528135815\n",
      "Epoch 89: Predicted Output = [0.44342429073448064, 0.4666297458860032], Loss = 0.23088651934501864\n",
      "Epoch 90: Predicted Output = [0.44343570789853354, 0.4665259952138164], Loss = 0.2309457732841942\n",
      "Epoch 91: Predicted Output = [0.44344699639912216, 0.4664231287193695], Loss = 0.2310045194137173\n",
      "Epoch 92: Predicted Output = [0.4434581574516886, 0.4663211424012291], Loss = 0.2310627600786833\n",
      "Epoch 93: Predicted Output = [0.44346919226638404, 0.4662200322053425], Loss = 0.23112049765352194\n",
      "Epoch 94: Predicted Output = [0.443480102047961, 0.46611979402668224], Loss = 0.23117773454107826\n",
      "Epoch 95: Predicted Output = [0.443490887995667, 0.4660204237108755], Loss = 0.23123447317170118\n",
      "Epoch 96: Predicted Output = [0.4435015513031359, 0.4659219170558104], Loss = 0.23129071600234108\n",
      "Epoch 97: Predicted Output = [0.44351209315828277, 0.4658242698132268], Loss = 0.23134646551565624\n",
      "Epoch 98: Predicted Output = [0.443522514743197, 0.4657274776902864], Loss = 0.23140172421912727\n",
      "Epoch 99: Predicted Output = [0.443532817234038, 0.46563153635112575], Loss = 0.23145649464418125\n",
      "Epoch 100: Predicted Output = [0.4435430018009321, 0.46553644141839007], Loss = 0.23151077934532444\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(inputs, h_output, o_output, target):\n",
    "    # Calculate the error in the output layer\n",
    "    o_error = [target[i] - o_output[i] for i in range(len(target))]\n",
    "\n",
    "    # Calculate the delta for the output layer\n",
    "    o_delta = [o_error[i] * relu_derivative(o_output[i]) for i in range(len(o_error))]\n",
    "\n",
    "    # Calculate the error in the hidden layer\n",
    "    h_error = [sum([o_delta[j] * wh[j][i] for j in range(len(o_delta))]) for i in range(len(h_output))]\n",
    "\n",
    "    # Calculate the delta for the hidden layer\n",
    "    h_delta = [h_error[i] * tanh_derivative(h_output[i]) for i in range(len(h_error))]\n",
    "\n",
    "    return o_delta, h_delta\n",
    "    \n",
    "    \n",
    "def update_weights(inputs, h_output, o_delta, h_delta, lr):\n",
    "    global w, wh, b, bh, wo, bo\n",
    "\n",
    "    # Update weights and biases in the output layer\n",
    "    wo = [[wo[i][j] + lr * o_delta[i] * h_output[j] for j in range(len(wo[0]))] for i in range(len(wo))]\n",
    "    bo = [bo[i] + lr * o_delta[i] for i in range(len(bo))]\n",
    "\n",
    "    # Update weights and biases in the hidden layer\n",
    "    wh = [[wh[i][j] + lr * h_delta[i] * h_output[j] for j in range(len(wh[0]))] for i in range(len(wh))]\n",
    "    bh = [bh[i] + lr * h_delta[i] for i in range(len(bh))]\n",
    "\n",
    "    # Update weights and biases in the input layer\n",
    "    w = [[w[i][j] + lr * inputs[j] * h_delta[i] for j in range(len(w[0]))] for i in range(len(w))]\n",
    "    b = [b[i] + lr * h_delta[i] for i in range(len(b))]\n",
    "\n",
    "\n",
    "def train(inputs, target, lr):\n",
    "    h_output, o_output = forward_pass(inputs)\n",
    "    o_delta, h_delta = backward_pass(inputs, h_output, o_output, target)\n",
    "    update_weights(inputs, h_output, o_delta, h_delta, lr)\n",
    "    \n",
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 100\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    \n",
    "    \n",
    "# Training Loop:\n",
    "for epoch in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        h_output, o_output = forward_pass(inputs)\n",
    "        print(f\"Epoch {epoch+1}: Predicted Output = {o_output}, Loss = {sum([(target[i] - o_output[i])**2 for i in range(len(target))])/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs):\n",
    "    # Calculate the inputs to the hidden layer\n",
    "    h_input = [sum([inputs[j] * w[i][j] for j in range(len(inputs))]) + b[i] for i in range(len(b))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the hidden layer\n",
    "    h_output = [tanh(x) for x in h_input]\n",
    "\n",
    "    # Calculate the inputs to the output layer\n",
    "    o_input = [sum([h_output[j] * wh[i][j] for j in range(len(h_output))]) + bh[i] for i in range(len(bh))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the output layer\n",
    "    o_output = [tanh(x) for x in o_input]\n",
    "\n",
    "    return h_output, o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Predicted Output = [0.4459098062021061, 0.4685057329801445], Loss = 0.23098681483886702\n",
      "Epoch 2: Predicted Output = [0.44592896342242705, 0.46852828881541286], Loss = 0.23098340335816658\n",
      "Epoch 3: Predicted Output = [0.44594804592140264, 0.46855072582422924], Loss = 0.2309800221405638\n",
      "Epoch 4: Predicted Output = [0.4459670539478235, 0.46857304442379943], Loss = 0.23097667106470474\n",
      "Epoch 5: Predicted Output = [0.445985987749823, 0.46859524503019107], Loss = 0.23097335000965763\n",
      "Epoch 6: Predicted Output = [0.4460048475748782, 0.4686173280583324], Loss = 0.2309700588549127\n",
      "Epoch 7: Predicted Output = [0.4460236336698095, 0.4686392939220155], Loss = 0.2309667974803794\n",
      "Epoch 8: Predicted Output = [0.44604234628078143, 0.468661143033896], Loss = 0.23096356576638633\n",
      "Epoch 9: Predicted Output = [0.44606098565330343, 0.4686828758054944], Loss = 0.23096036359368002\n",
      "Epoch 10: Predicted Output = [0.44607955203222976, 0.46870449264719666], Loss = 0.23095719084342345\n",
      "Epoch 11: Predicted Output = [0.44609804566175965, 0.4687259939682563], Loss = 0.23095404739719427\n",
      "Epoch 12: Predicted Output = [0.4461164667854392, 0.4687473801767947], Loss = 0.230950933136985\n",
      "Epoch 13: Predicted Output = [0.44613481564616037, 0.4687686516798021], Loss = 0.2309478479452009\n",
      "Epoch 14: Predicted Output = [0.44615309248616214, 0.4687898088831394], Loss = 0.23094479170465848\n",
      "Epoch 15: Predicted Output = [0.44617129754703105, 0.4688108521915387], Loss = 0.23094176429858543\n",
      "Epoch 16: Predicted Output = [0.44618943106970127, 0.4688317820086045], Loss = 0.23093876561061816\n",
      "Epoch 17: Predicted Output = [0.4462074932944563, 0.4688525987368152], Loss = 0.23093579552480203\n",
      "Epoch 18: Predicted Output = [0.4462254844609275, 0.4688733027775238], Loss = 0.23093285392558863\n",
      "Epoch 19: Predicted Output = [0.44624340480809693, 0.46889389453096014], Loss = 0.23092994069783557\n",
      "Epoch 20: Predicted Output = [0.44626125457429616, 0.46891437439623024], Loss = 0.23092705572680544\n",
      "Epoch 21: Predicted Output = [0.44627903399720775, 0.4689347427713195], Loss = 0.23092419889816385\n",
      "Epoch 22: Predicted Output = [0.4462967433138655, 0.4689550000530927], Loss = 0.23092137009797883\n",
      "Epoch 23: Predicted Output = [0.44631438276065505, 0.46897514663729617], Loss = 0.2309185692127192\n",
      "Epoch 24: Predicted Output = [0.44633195257331465, 0.4689951829185576], Loss = 0.23091579612925428\n",
      "Epoch 25: Predicted Output = [0.4463494529869359, 0.46901510929038925], Loss = 0.2309130507348516\n",
      "Epoch 26: Predicted Output = [0.44636688423596355, 0.4690349261451877], Loss = 0.2309103329171764\n",
      "Epoch 27: Predicted Output = [0.4463842465541975, 0.46905463387423607], Loss = 0.23090764256429044\n",
      "Epoch 28: Predicted Output = [0.4464015401747923, 0.4690742328677051], Loss = 0.23090497956465036\n",
      "Epoch 29: Predicted Output = [0.44641876533025815, 0.4690937235146544], Loss = 0.2309023438071071\n",
      "Epoch 30: Predicted Output = [0.44643592225246187, 0.4691131062030337], Loss = 0.23089973518090448\n",
      "Epoch 31: Predicted Output = [0.4464530111726269, 0.469132381319685], Loss = 0.23089715357567758\n",
      "Epoch 32: Predicted Output = [0.446470032321335, 0.4691515492503425], Loss = 0.23089459888145275\n",
      "Epoch 33: Predicted Output = [0.44648698592852565, 0.46917061037963576], Loss = 0.23089207098864503\n",
      "Epoch 34: Predicted Output = [0.4465038722234981, 0.4691895650910899], Loss = 0.23088956978805802\n",
      "Epoch 35: Predicted Output = [0.44652069143491097, 0.4692084137671273], Loss = 0.2308870951708822\n",
      "Epoch 36: Predicted Output = [0.4465374437907837, 0.4692271567890695], Loss = 0.230884647028694\n",
      "Epoch 37: Predicted Output = [0.4465541295184966, 0.469245794537138], Loss = 0.23088222525345448\n",
      "Epoch 38: Predicted Output = [0.44657074884479286, 0.46926432739045654], Loss = 0.23087982973750842\n",
      "Epoch 39: Predicted Output = [0.4465873019957773, 0.4692827557270515], Loss = 0.2308774603735827\n",
      "Epoch 40: Predicted Output = [0.4466037891969192, 0.46930107992385445], Loss = 0.23087511705478606\n",
      "Epoch 41: Predicted Output = [0.44662021067305147, 0.4693193003567033], Loss = 0.2308727996746064\n",
      "Epoch 42: Predicted Output = [0.4466365666483725, 0.4693374174003436], Loss = 0.2308705081269113\n",
      "Epoch 43: Predicted Output = [0.44665285734644605, 0.4693554314284303], Loss = 0.23086824230594585\n",
      "Epoch 44: Predicted Output = [0.44666908299020325, 0.4693733428135294], Loss = 0.2308660021063319\n",
      "Epoch 45: Predicted Output = [0.4466852438019418, 0.4693911519271192], Loss = 0.2308637874230666\n",
      "Epoch 46: Predicted Output = [0.4467013400033284, 0.46940885913959235], Loss = 0.23086159815152169\n",
      "Epoch 47: Predicted Output = [0.44671737181539795, 0.46942646482025663], Loss = 0.230859434187442\n",
      "Epoch 48: Predicted Output = [0.4467333394585559, 0.46944396933733795], Loss = 0.23085729542694428\n",
      "Epoch 49: Predicted Output = [0.446749243152578, 0.4694613730579803], Loss = 0.23085518176651643\n",
      "Epoch 50: Predicted Output = [0.4467650831166116, 0.4694786763482487], Loss = 0.23085309310301594\n",
      "Epoch 51: Predicted Output = [0.44678085956917646, 0.4694958795731301], Loss = 0.23085102933366905\n",
      "Epoch 52: Predicted Output = [0.44679657272816553, 0.4695129830965353], Loss = 0.2308489903560696\n",
      "Epoch 53: Predicted Output = [0.44681222281084576, 0.4695299872813004], Loss = 0.23084697606817767\n",
      "Epoch 54: Predicted Output = [0.4468278100338592, 0.46954689248918896], Loss = 0.23084498636831857\n",
      "Epoch 55: Predicted Output = [0.4468433346132234, 0.469563699080893], Loss = 0.23084302115518196\n",
      "Epoch 56: Predicted Output = [0.4468587967643327, 0.4695804074160356], Loss = 0.23084108032782003\n",
      "Epoch 57: Predicted Output = [0.4468741967019592, 0.4695970178531712], Loss = 0.23083916378564737\n",
      "Epoch 58: Predicted Output = [0.44688953464025327, 0.4696135307497889], Loss = 0.230837271428439\n",
      "Epoch 59: Predicted Output = [0.4469048107927448, 0.4696299464623131], Loss = 0.23083540315632967\n",
      "Epoch 60: Predicted Output = [0.44692002537234343, 0.46964626534610593], Loss = 0.23083355886981216\n",
      "Epoch 61: Predicted Output = [0.4469351785913408, 0.46966248775546804], Loss = 0.23083173846973765\n",
      "Epoch 62: Predicted Output = [0.44695027066141, 0.4696786140436413], Loss = 0.2308299418573127\n",
      "Epoch 63: Predicted Output = [0.4469653017936073, 0.46969464456281035], Loss = 0.2308281689340993\n",
      "Epoch 64: Predicted Output = [0.44698027219837316, 0.469710579664104], Loss = 0.2308264196020135\n",
      "Epoch 65: Predicted Output = [0.44699518208553246, 0.4697264196975973], Loss = 0.23082469376332415\n",
      "Epoch 66: Predicted Output = [0.44701003166429654, 0.4697421650123134], Loss = 0.23082299132065215\n",
      "Epoch 67: Predicted Output = [0.4470248211432628, 0.46975781595622484], Loss = 0.23082131217696905\n",
      "Epoch 68: Predicted Output = [0.44703955073041707, 0.46977337287625626], Loss = 0.2308196562355957\n",
      "Epoch 69: Predicted Output = [0.4470542206331334, 0.46978883611828537], Loss = 0.2308180234002019\n",
      "Epoch 70: Predicted Output = [0.4470688310581757, 0.46980420602714523], Loss = 0.23081641357480442\n",
      "Epoch 71: Predicted Output = [0.4470833822116987, 0.46981948294662573], Loss = 0.2308148266637668\n",
      "Epoch 72: Predicted Output = [0.44709787429924824, 0.4698346672194759], Loss = 0.23081326257179735\n",
      "Epoch 73: Predicted Output = [0.4471123075257637, 0.4698497591874054], Loss = 0.230811721203949\n",
      "Epoch 74: Predicted Output = [0.4471266820955769, 0.4698647591910865], Loss = 0.23081020246561695\n",
      "Epoch 75: Predicted Output = [0.44714099821241504, 0.4698796675701559], Loss = 0.23080870626253905\n",
      "Epoch 76: Predicted Output = [0.44715525607940093, 0.46989448466321687], Loss = 0.23080723250079366\n",
      "Epoch 77: Predicted Output = [0.44716945589905344, 0.46990921080784037], Loss = 0.2308057810867989\n",
      "Epoch 78: Predicted Output = [0.44718359787328993, 0.469923846340568], Loss = 0.2308043519273118\n",
      "Epoch 79: Predicted Output = [0.4471976822034255, 0.469938391596913], Loss = 0.2308029449294266\n",
      "Epoch 80: Predicted Output = [0.4472117090901754, 0.4699528469113628], Loss = 0.23080156000057434\n",
      "Epoch 81: Predicted Output = [0.4472256787336557, 0.4699672126173802], Loss = 0.23080019704852148\n",
      "Epoch 82: Predicted Output = [0.4472395913333838, 0.4699814890474065], Loss = 0.2307988559813685\n",
      "Epoch 83: Predicted Output = [0.44725344708828013, 0.4699956765328618], Loss = 0.23079753670754974\n",
      "Epoch 84: Predicted Output = [0.4472672461966691, 0.4700097754041481], Loss = 0.23079623913583144\n",
      "Epoch 85: Predicted Output = [0.4472809888562795, 0.4700237859906514], Loss = 0.23079496317531073\n",
      "Epoch 86: Predicted Output = [0.4472946752642463, 0.4700377086207423], Loss = 0.23079370873541533\n",
      "Epoch 87: Predicted Output = [0.44730830561711166, 0.47005154362177926], Loss = 0.23079247572590184\n",
      "Epoch 88: Predicted Output = [0.44732188011082524, 0.47006529132011055], Loss = 0.23079126405685424\n",
      "Epoch 89: Predicted Output = [0.4473353989407465, 0.47007895204107486], Loss = 0.23079007363868442\n",
      "Epoch 90: Predicted Output = [0.4473488623016444, 0.4700925261090049], Loss = 0.23078890438212918\n",
      "Epoch 91: Predicted Output = [0.4473622703876993, 0.4701060138472282], Loss = 0.23078775619825076\n",
      "Epoch 92: Predicted Output = [0.44737562339250425, 0.4701194155780701], Loss = 0.23078662899843452\n",
      "Epoch 93: Predicted Output = [0.44738892150906545, 0.4701327316228547], Loss = 0.23078552269438907\n",
      "Epoch 94: Predicted Output = [0.44740216492980356, 0.47014596230190747], Loss = 0.2307844371981444\n",
      "Epoch 95: Predicted Output = [0.44741535384655495, 0.4701591079345571], Loss = 0.2307833724220512\n",
      "Epoch 96: Predicted Output = [0.44742848845057226, 0.47017216883913765], Loss = 0.23078232827877923\n",
      "Epoch 97: Predicted Output = [0.4474415689325267, 0.4701851453329904], Loss = 0.2307813046813174\n",
      "Epoch 98: Predicted Output = [0.4474545954825074, 0.47019803773246577], Loss = 0.23078030154297163\n",
      "Epoch 99: Predicted Output = [0.44746756829002426, 0.4702108463529256], Loss = 0.2307793187773645\n",
      "Epoch 100: Predicted Output = [0.44748048754400804, 0.47022357150874494], Loss = 0.23077835629843388\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(inputs, h_output, o_output, target):\n",
    "    # Calculate the error in the output layer\n",
    "    o_error = [target[i] - o_output[i] for i in range(len(target))]\n",
    "\n",
    "    # Calculate the delta for the output layer\n",
    "    o_delta = [o_error[i] * sigmoid_derivative(o_output[i]) for i in range(len(o_error))]\n",
    "\n",
    "    # Calculate the error in the hidden layer\n",
    "    h_error = [sum([o_delta[j] * wh[j][i] for j in range(len(o_delta))]) for i in range(len(h_output))]\n",
    "\n",
    "    # Calculate the delta for the hidden layer\n",
    "    h_delta = [h_error[i] * sigmoid_derivative(h_output[i]) for i in range(len(h_error))]\n",
    "\n",
    "    return o_delta, h_delta\n",
    "    \n",
    "    \n",
    "def update_weights(inputs, h_output, o_delta, h_delta, lr):\n",
    "    global w, wh, b, bh, wo, bo\n",
    "\n",
    "    # Update weights and biases in the output layer\n",
    "    wo = [[wo[i][j] + lr * o_delta[i] * h_output[j] for j in range(len(wo[0]))] for i in range(len(wo))]\n",
    "    bo = [bo[i] + lr * o_delta[i] for i in range(len(bo))]\n",
    "\n",
    "    # Update weights and biases in the hidden layer\n",
    "    wh = [[wh[i][j] + lr * h_delta[i] * h_output[j] for j in range(len(wh[0]))] for i in range(len(wh))]\n",
    "    bh = [bh[i] + lr * h_delta[i] for i in range(len(bh))]\n",
    "\n",
    "    # Update weights and biases in the input layer\n",
    "    w = [[w[i][j] + lr * inputs[j] * h_delta[i] for j in range(len(w[0]))] for i in range(len(w))]\n",
    "    b = [b[i] + lr * h_delta[i] for i in range(len(b))]\n",
    "\n",
    "\n",
    "def train(inputs, target, lr):\n",
    "    h_output, o_output = forward_pass(inputs)\n",
    "    o_delta, h_delta = backward_pass(inputs, h_output, o_output, target)\n",
    "    update_weights(inputs, h_output, o_delta, h_delta, lr)\n",
    "    \n",
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 100\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    \n",
    "    \n",
    "# Training Loop:\n",
    "for epoch in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        h_output, o_output = forward_pass(inputs)\n",
    "        print(f\"Epoch {epoch+1}: Predicted Output = {o_output}, Loss = {sum([(target[i] - o_output[i])**2 for i in range(len(target))])/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    # Derivative of softmax is calculated separately during backpropagation\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
