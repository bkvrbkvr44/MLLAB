{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer weights and biases\n",
    "w = [[0.15, 0.2], [0.25, 0.3]]\n",
    "b = [0.35, 0.35]\n",
    "\n",
    "# Hidden layer weights and biases\n",
    "wh = [[0.4, 0.45], [0.5, 0.55]]\n",
    "bh = [0.6, 0.6]\n",
    "\n",
    "# Output layer weights and biases\n",
    "wo = [[0.01, 0.99], [0.01, 0.99]]\n",
    "bo = [0.01, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs):\n",
    "    # Calculate the inputs to the hidden layer\n",
    "    h_input = [sum([inputs[j] * w[i][j] for j in range(len(inputs))]) + b[i] for i in range(len(b))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the hidden layer\n",
    "    h_output = [sigmoid(x) for x in h_input]\n",
    "\n",
    "    # Calculate the inputs to the output layer\n",
    "    o_input = [sum([h_output[j] * wh[i][j] for j in range(len(h_output))]) + bh[i] for i in range(len(bh))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the output layer\n",
    "    o_output = [sigmoid(x) for x in o_input]\n",
    "\n",
    "    return h_output, o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(inputs, h_output, o_output, target):\n",
    "    # Calculate the error in the output layer\n",
    "    o_error = [target[i] - o_output[i] for i in range(len(target))]\n",
    "\n",
    "    # Calculate the delta for the output layer\n",
    "    o_delta = [o_error[i] * sigmoid_derivative(o_output[i]) for i in range(len(o_error))]\n",
    "\n",
    "    # Calculate the error in the hidden layer\n",
    "    h_error = [sum([o_delta[j] * wh[j][i] for j in range(len(o_delta))]) for i in range(len(h_output))]\n",
    "\n",
    "    # Calculate the delta for the hidden layer\n",
    "    h_delta = [h_error[i] * sigmoid_derivative(h_output[i]) for i in range(len(h_error))]\n",
    "\n",
    "    return o_delta, h_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(inputs, h_output, o_delta, h_delta, lr):\n",
    "    global w, wh, b, bh, wo, bo\n",
    "\n",
    "    # Update weights and biases in the output layer\n",
    "    wo = [[wo[i][j] + lr * o_delta[i] * h_output[j] for j in range(len(wo[0]))] for i in range(len(wo))]\n",
    "    bo = [bo[i] + lr * o_delta[i] for i in range(len(bo))]\n",
    "\n",
    "    # Update weights and biases in the hidden layer\n",
    "    wh = [[wh[i][j] + lr * h_delta[i] * h_output[j] for j in range(len(wh[0]))] for i in range(len(wh))]\n",
    "    bh = [bh[i] + lr * h_delta[i] for i in range(len(bh))]\n",
    "\n",
    "    # Update weights and biases in the input layer\n",
    "    w = [[w[i][j] + lr * inputs[j] * h_delta[i] for j in range(len(w[0]))] for i in range(len(w))]\n",
    "    b = [b[i] + lr * h_delta[i] for i in range(len(b))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, target, lr):\n",
    "    h_output, o_output = forward_pass(inputs)\n",
    "    o_delta, h_delta = backward_pass(inputs, h_output, o_output, target)\n",
    "    update_weights(inputs, h_output, o_delta, h_delta, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 10000\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: Predicted Output = [0.5386244805837684, 0.49459472193356097], Loss = 0.2624351155042724\n",
      "Epoch 2000: Predicted Output = [0.538624480567295, 0.4945947219126518], Loss = 0.2624351155059226\n",
      "Epoch 3000: Predicted Output = [0.5386244805683764, 0.49459472191468695], Loss = 0.26243511550548604\n",
      "Epoch 4000: Predicted Output = [0.538624480568447, 0.49459472191464543], Loss = 0.262435115505544\n",
      "Epoch 5000: Predicted Output = [0.5386244805684105, 0.4945947219146125], Loss = 0.26243511550554094\n",
      "Epoch 6000: Predicted Output = [0.5386244805684072, 0.49459472191461523], Loss = 0.26243511550553783\n",
      "Epoch 7000: Predicted Output = [0.5386244805684036, 0.49459472191461806], Loss = 0.26243511550553456\n",
      "Epoch 8000: Predicted Output = [0.5386244805684002, 0.49459472191462067], Loss = 0.26243511550553145\n",
      "Epoch 9000: Predicted Output = [0.5386244805683967, 0.4945947219146235], Loss = 0.2624351155055282\n",
      "Epoch 10000: Predicted Output = [0.5386244805683933, 0.4945947219146262], Loss = 0.26243511550552506\n"
     ]
    }
   ],
   "source": [
    "# Training Loop:\n",
    "for epoch in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        h_output, o_output = forward_pass(inputs)\n",
    "        print(f\"Epoch {epoch+1}: Predicted Output = {o_output}, Loss = {sum([(target[i] - o_output[i])**2 for i in range(len(target))])/2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Error after first round of Backpropagation: 0.26243511550552506\n"
     ]
    }
   ],
   "source": [
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 1\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "\n",
    "h_output, o_output = forward_pass(inputs)\n",
    "error = sum([(target[i] - o_output[i])**2 for i in range(len(target))]) / 2\n",
    "print(f\"Total Error after first round of Backpropagation: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "\n",
    "def forward_pass(inputs):\n",
    "    # Calculate the inputs to the hidden layer\n",
    "    h_input = [sum([inputs[j] * w[i][j] for j in range(len(inputs))]) + b[i] for i in range(len(b))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the hidden layer\n",
    "    h_output = [relu(x) for x in h_input]\n",
    "\n",
    "    # Calculate the inputs to the output layer\n",
    "    o_input = [sum([h_output[j] * wh[i][j] for j in range(len(h_output))]) + bh[i] for i in range(len(bh))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the output layer\n",
    "    o_output = [relu(x) for x in o_input]\n",
    "\n",
    "    return h_output, o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Predicted Output = [0.4426285911991529, 0.4605353741099629], Loss = 0.2337501439959203\n",
      "Epoch 2: Predicted Output = [0.44262831530731606, 0.4605673163147476], Loss = 0.23373311287990745\n",
      "Epoch 3: Predicted Output = [0.4426280419415279, 0.46059885891122376], Loss = 0.2337162954301294\n",
      "Epoch 4: Predicted Output = [0.44262777095817496, 0.46063000702788603], Loss = 0.23369968883176753\n",
      "Epoch 5: Predicted Output = [0.4426275022339127, 0.46066076574443837], Loss = 0.23368329030570922\n",
      "Epoch 6: Predicted Output = [0.4426272356630926, 0.46069114008939926], Loss = 0.23366709710867448\n",
      "Epoch 7: Predicted Output = [0.4426269711554995, 0.4607211350381276], Loss = 0.23365110653325466\n",
      "Epoch 8: Predicted Output = [0.4426267086343616, 0.4607507555112186], Loss = 0.23363531590787334\n",
      "Epoch 9: Predicted Output = [0.4426264480346019, 0.4607800063732208], Loss = 0.23361972259668215\n",
      "Epoch 10: Predicted Output = [0.44262618930130126, 0.4608088924316364], Loss = 0.23360432399939832\n",
      "Epoch 11: Predicted Output = [0.44262593238834896, 0.460837418436166], Loss = 0.2335891175510948\n",
      "Epoch 12: Predicted Output = [0.4426256772572572, 0.46086558907816866], Loss = 0.23357410072194695\n",
      "Epoch 13: Predicted Output = [0.44262542387612147, 0.46089340899030806], Loss = 0.23355927101694557\n",
      "Epoch 14: Predicted Output = [0.44262517221870795, 0.4609208827463609], Loss = 0.23354462597557835\n",
      "Epoch 15: Predicted Output = [0.44262492226365424, 0.4609480148611655], Loss = 0.2335301631714872\n",
      "Epoch 16: Predicted Output = [0.44262467399376837, 0.4609748097906925], Loss = 0.23351588021210418\n",
      "Epoch 17: Predicted Output = [0.44262442739541574, 0.46100127193222007], Loss = 0.23350177473827016\n",
      "Epoch 18: Predicted Output = [0.4426241824579821, 0.46102740562459976], Loss = 0.23348784442383952\n",
      "Epoch 19: Predicted Output = [0.442623939173405, 0.46105321514859987], Loss = 0.2334740869752737\n",
      "Epoch 20: Predicted Output = [0.4426236975357638, 0.46107870472731505], Loss = 0.23346050013122543\n",
      "Epoch 21: Predicted Output = [0.4426234575409228, 0.4611038785266321], Loss = 0.23344708166211703\n",
      "Epoch 22: Predicted Output = [0.44262321918621944, 0.46112874065574394], Loss = 0.23343382936971352\n",
      "Epoch 23: Predicted Output = [0.44262298247019394, 0.46115329516770304], Loss = 0.23342074108669214\n",
      "Epoch 24: Predicted Output = [0.44262274739235374, 0.4611775460600087], Loss = 0.23340781467621124\n",
      "Epoch 25: Predicted Output = [0.4426225139529694, 0.4612014972752221], Loss = 0.2333950480314771\n",
      "Epoch 26: Predicted Output = [0.44262228215289806, 0.46122515270160297], Loss = 0.23338243907531242\n",
      "Epoch 27: Predicted Output = [0.4426220519934308, 0.4612485161737657], Loss = 0.23336998575972562\n",
      "Epoch 28: Predicted Output = [0.4426218234761612, 0.4612715914733482], Loss = 0.23335768606548238\n",
      "Epoch 29: Predicted Output = [0.44262159660287226, 0.46129438232969244], Loss = 0.23334553800167987\n",
      "Epoch 30: Predicted Output = [0.44262137137544, 0.46131689242053253], Loss = 0.23333353960532455\n",
      "Epoch 31: Predicted Output = [0.4426211477957509, 0.461339125372688], Loss = 0.23332168894091376\n",
      "Epoch 32: Predicted Output = [0.44262092586563184, 0.4613610847627603], Loss = 0.23330998410002102\n",
      "Epoch 33: Predicted Output = [0.442620705586791, 0.4613827741178298], Loss = 0.23329842320088712\n",
      "Epoch 34: Predicted Output = [0.4426204869607685, 0.4614041969161533], Loss = 0.23328700438801464\n",
      "Epoch 35: Predicted Output = [0.4426202699888949, 0.46142535658785877], Loss = 0.2332757258317683\n",
      "Epoch 36: Predicted Output = [0.44262005467225723, 0.46144625651563653], Loss = 0.23326458572798056\n",
      "Epoch 37: Predicted Output = [0.44261984101167157, 0.46146690003542734], Loss = 0.23325358229756246\n",
      "Epoch 38: Predicted Output = [0.4426196290076605, 0.46148729043710424], Loss = 0.2332427137861198\n",
      "Epoch 39: Predicted Output = [0.4426194186604361, 0.4615074309651487], Loss = 0.23323197846357538\n",
      "Epoch 40: Predicted Output = [0.44261920996988624, 0.4615273248193206], Loss = 0.23322137462379622\n",
      "Epoch 41: Predicted Output = [0.4426190029355654, 0.4615469751553205], Loss = 0.23321090058422708\n",
      "Epoch 42: Predicted Output = [0.44261879755668776, 0.4615663850854447], Loss = 0.23320055468552947\n",
      "Epoch 43: Predicted Output = [0.4426185938321233, 0.46158555767923237], Loss = 0.23319033529122574\n",
      "Epoch 44: Predicted Output = [0.4426183917603964, 0.46160449596410413], Loss = 0.2331802407873501\n",
      "Epoch 45: Predicted Output = [0.44261819133968605, 0.46162320292599274], Loss = 0.2331702695821039\n",
      "Epoch 46: Predicted Output = [0.44261799256782774, 0.46164168150996415], Loss = 0.23316042010551757\n",
      "Epoch 47: Predicted Output = [0.44261779544231716, 0.46165993462083105], Loss = 0.23315069080911757\n",
      "Epoch 48: Predicted Output = [0.4426175999603148, 0.46167796512375636], Loss = 0.23314108016559887\n",
      "Epoch 49: Predicted Output = [0.44261740611865125, 0.4616957758448484], Loss = 0.23313158666850337\n",
      "Epoch 50: Predicted Output = [0.442617213913834, 0.4617133695717472], Loss = 0.23312220883190266\n",
      "Epoch 51: Predicted Output = [0.4426170233420547, 0.4617307490542012], Loss = 0.23311294519008763\n",
      "Epoch 52: Predicted Output = [0.4426168343991966, 0.4617479170046357], Loss = 0.23310379429726158\n",
      "Epoch 53: Predicted Output = [0.44261664708084264, 0.4617648760987118], Loss = 0.23309475472723978\n",
      "Epoch 54: Predicted Output = [0.44261646138228444, 0.4617816289758769], Loss = 0.23308582507315387\n",
      "Epoch 55: Predicted Output = [0.44261627729853054, 0.4617981782399059], Loss = 0.23307700394716063\n",
      "Epoch 56: Predicted Output = [0.44261609482431585, 0.4618145264594345], Loss = 0.23306828998015644\n",
      "Epoch 57: Predicted Output = [0.4426159139541103, 0.46183067616848283], Loss = 0.2330596818214961\n",
      "Epoch 58: Predicted Output = [0.44261573468212845, 0.46184662986697134], Loss = 0.23305117813871687\n",
      "Epoch 59: Predicted Output = [0.44261555700233823, 0.4618623900212279], Loss = 0.23304277761726644\n",
      "Epoch 60: Predicted Output = [0.44261538090847075, 0.46187795906448664], Loss = 0.23303447896023666\n",
      "Epoch 61: Predicted Output = [0.4426152063940289, 0.46189333939737853], Loss = 0.23302628088810035\n",
      "Epoch 62: Predicted Output = [0.4426150334522965, 0.461908533388414], Loss = 0.23301818213845377\n",
      "Epoch 63: Predicted Output = [0.4426148620763477, 0.4619235433744573], Loss = 0.23301018146576302\n",
      "Epoch 64: Predicted Output = [0.44261469225905525, 0.4619383716611937], Loss = 0.2330022776411143\n",
      "Epoch 65: Predicted Output = [0.44261452399309975, 0.4619530205235879], Loss = 0.23299446945196933\n",
      "Epoch 66: Predicted Output = [0.4426143572709778, 0.46196749220633576], Loss = 0.23298675570192368\n",
      "Epoch 67: Predicted Output = [0.4426141920850107, 0.46198178892430825], Loss = 0.23297913521047012\n",
      "Epoch 68: Predicted Output = [0.4426140284273524, 0.4619959128629878], Loss = 0.2329716068127658\n",
      "Epoch 69: Predicted Output = [0.44261386628999794, 0.46200986617889783], Loss = 0.23296416935940278\n",
      "Epoch 70: Predicted Output = [0.44261370566479075, 0.46202365100002507], Loss = 0.23295682171618276\n",
      "Epoch 71: Predicted Output = [0.44261354654343105, 0.46203726942623485], Loss = 0.23294956276389572\n",
      "Epoch 72: Predicted Output = [0.44261338891748264, 0.46205072352967924], Loss = 0.23294239139810213\n",
      "Epoch 73: Predicted Output = [0.44261323277838066, 0.4620640153551992], Loss = 0.23293530652891836\n",
      "Epoch 74: Predicted Output = [0.44261307811743844, 0.46207714692071955], Loss = 0.2329283070808062\n",
      "Epoch 75: Predicted Output = [0.4426129249258545, 0.4620901202176377], Loss = 0.2329213919923656\n",
      "Epoch 76: Predicted Output = [0.4426127731947193, 0.46210293721120554], Loss = 0.232914560216131\n",
      "Epoch 77: Predicted Output = [0.4426126229150217, 0.46211559984090606], Loss = 0.23290781071837055\n",
      "Epoch 78: Predicted Output = [0.4426124740776551, 0.4621281100208228], Loss = 0.2329011424788892\n",
      "Epoch 79: Predicted Output = [0.44261232667342393, 0.46214046964000405], Loss = 0.2328945544908344\n",
      "Epoch 80: Predicted Output = [0.4426121806930494, 0.4621526805628204], Loss = 0.23288804576050576\n",
      "Epoch 81: Predicted Output = [0.4426120361271752, 0.4621647446293173], Loss = 0.23288161530716706\n",
      "Epoch 82: Predicted Output = [0.44261189296637327, 0.4621766636555614], Loss = 0.23287526216286156\n",
      "Epoch 83: Predicted Output = [0.4426117512011491, 0.4621884394339813], Loss = 0.2328689853722305\n",
      "Epoch 84: Predicted Output = [0.4426116108219468, 0.4622000737337031], Loss = 0.232862783992334\n",
      "Epoch 85: Predicted Output = [0.4426114718191546, 0.4622115683008801], Loss = 0.23285665709247588\n",
      "Epoch 86: Predicted Output = [0.4426113341831093, 0.462222924859018], Loss = 0.23285060375402983\n",
      "Epoch 87: Predicted Output = [0.4426111979041013, 0.462234145109294], Loss = 0.23284462307026965\n",
      "Epoch 88: Predicted Output = [0.442611062972379, 0.46224523073087126], Loss = 0.23283871414620144\n",
      "Epoch 89: Predicted Output = [0.4426109293781534, 0.46225618338120855], Loss = 0.2328328760983991\n",
      "Epoch 90: Predicted Output = [0.4426107971116021, 0.46226700469636445], Loss = 0.23282710805484141\n",
      "Epoch 91: Predicted Output = [0.4426106661628738, 0.4622776962912972], Loss = 0.23282140915475288\n",
      "Epoch 92: Predicted Output = [0.44261053652209204, 0.46228825976015986], Loss = 0.23281577854844643\n",
      "Epoch 93: Predicted Output = [0.442610408179359, 0.46229869667659], Loss = 0.23281021539716854\n",
      "Epoch 94: Predicted Output = [0.44261028112475936, 0.46230900859399604], Loss = 0.23280471887294735\n",
      "Epoch 95: Predicted Output = [0.44261015534836384, 0.46231919704583824], Loss = 0.2327992881584422\n",
      "Epoch 96: Predicted Output = [0.44261003084023265, 0.4623292635459056], Loss = 0.23279392244679672\n",
      "Epoch 97: Predicted Output = [0.44260990759041874, 0.4623392095885883], Loss = 0.23278862094149316\n",
      "Epoch 98: Predicted Output = [0.442609785588971, 0.4623490366491462], Loss = 0.23278338285620978\n",
      "Epoch 99: Predicted Output = [0.44260966482593767, 0.46235874618397266], Loss = 0.23277820741467975\n",
      "Epoch 100: Predicted Output = [0.44260954529136864, 0.46236833963085505], Loss = 0.2327730938505527\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(inputs, h_output, o_output, target):\n",
    "    # Calculate the error in the output layer\n",
    "    o_error = [target[i] - o_output[i] for i in range(len(target))]\n",
    "\n",
    "    # Calculate the delta for the output layer\n",
    "    o_delta = [o_error[i] * relu_derivative(o_output[i]) for i in range(len(o_error))]\n",
    "\n",
    "    # Calculate the error in the hidden layer\n",
    "    h_error = [sum([o_delta[j] * wh[j][i] for j in range(len(o_delta))]) for i in range(len(h_output))]\n",
    "\n",
    "    # Calculate the delta for the hidden layer\n",
    "    h_delta = [h_error[i] * relu_derivative(h_output[i]) for i in range(len(h_error))]\n",
    "\n",
    "    return o_delta, h_delta\n",
    "    \n",
    "    \n",
    "def update_weights(inputs, h_output, o_delta, h_delta, lr):\n",
    "    global w, wh, b, bh, wo, bo\n",
    "\n",
    "    # Update weights and biases in the output layer\n",
    "    wo = [[wo[i][j] + lr * o_delta[i] * h_output[j] for j in range(len(wo[0]))] for i in range(len(wo))]\n",
    "    bo = [bo[i] + lr * o_delta[i] for i in range(len(bo))]\n",
    "\n",
    "    # Update weights and biases in the hidden layer\n",
    "    wh = [[wh[i][j] + lr * h_delta[i] * h_output[j] for j in range(len(wh[0]))] for i in range(len(wh))]\n",
    "    bh = [bh[i] + lr * h_delta[i] for i in range(len(bh))]\n",
    "\n",
    "    # Update weights and biases in the input layer\n",
    "    w = [[w[i][j] + lr * inputs[j] * h_delta[i] for j in range(len(w[0]))] for i in range(len(w))]\n",
    "    b = [b[i] + lr * h_delta[i] for i in range(len(b))]\n",
    "\n",
    "\n",
    "def train(inputs, target, lr):\n",
    "    h_output, o_output = forward_pass(inputs)\n",
    "    o_delta, h_delta = backward_pass(inputs, h_output, o_output, target)\n",
    "    update_weights(inputs, h_output, o_delta, h_delta, lr)\n",
    "    \n",
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 100\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    \n",
    "    \n",
    "# Training Loop:\n",
    "for epoch in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        h_output, o_output = forward_pass(inputs)\n",
    "        print(f\"Epoch {epoch+1}: Predicted Output = {o_output}, Loss = {sum([(target[i] - o_output[i])**2 for i in range(len(target))])/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - math.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Predicted Output = [0.4434204259886967, 0.4614171649551037], Loss = 0.23362653958416163\n",
      "Epoch 2: Predicted Output = [0.44342539440496154, 0.46136951011157173], Loss = 0.23365388367738804\n",
      "Epoch 3: Predicted Output = [0.4434302733438423, 0.4613222981574478], Loss = 0.23368095713822015\n",
      "Epoch 4: Predicted Output = [0.44343506631342655, 0.46127552891901963], Loss = 0.23370776151499348\n",
      "Epoch 5: Predicted Output = [0.44343977658264877, 0.46122920191608446], Loss = 0.23373429841515875\n",
      "Epoch 6: Predicted Output = [0.44344440720008993, 0.4611833163884007], Loss = 0.23376056949950383\n",
      "Epoch 7: Predicted Output = [0.4434489610112645, 0.4611378713200263], Loss = 0.23378657647682888\n",
      "Epoch 8: Predicted Output = [0.44345344067451486, 0.46109286546171124], Loss = 0.23381232109903932\n",
      "Epoch 9: Predicted Output = [0.44345784867562754, 0.4610482973515031], Loss = 0.23383780515662356\n",
      "Epoch 10: Predicted Output = [0.4434621873412745, 0.46100416533370514], Loss = 0.23386303047448603\n",
      "Epoch 11: Predicted Output = [0.4434664588513736, 0.4609604675763239], Loss = 0.23388799890810566\n",
      "Epoch 12: Predicted Output = [0.4434706652504563, 0.4609172020871231], Loss = 0.23391271233999567\n",
      "Epoch 13: Predicted Output = [0.4434748084581222, 0.46087436672839677], Loss = 0.23393717267644043\n",
      "Epoch 14: Predicted Output = [0.44347889027865556, 0.46083195923056636], Loss = 0.23396138184448786\n",
      "Epoch 15: Predicted Output = [0.44348291240986937, 0.4607899772046917], Loss = 0.23398534178917657\n",
      "Epoch 16: Predicted Output = [0.44348687645124163, 0.460748418153987], Loss = 0.23400905447098053\n",
      "Epoch 17: Predicted Output = [0.4434907839113991, 0.4607072794844172], Loss = 0.23403252186345314\n",
      "Epoch 18: Predicted Output = [0.44349463621500196, 0.46066655851445276], Loss = 0.2340557459510551\n",
      "Epoch 19: Predicted Output = [0.4434984347090792, 0.4606262524840455], Loss = 0.2340787287271537\n",
      "Epoch 20: Predicted Output = [0.443502180668855, 0.46058635856288976], Loss = 0.23410147219217686\n",
      "Epoch 21: Predicted Output = [0.4435058753031127, 0.4605468738580273], Loss = 0.2341239783519128\n",
      "Epoch 22: Predicted Output = [0.4435095197591285, 0.4605077954208448], Loss = 0.2341462492159421\n",
      "Epoch 23: Predicted Output = [0.4435131151272124, 0.4604691202535147], Loss = 0.23416828679619317\n",
      "Epoch 24: Predicted Output = [0.44351666244488935, 0.46043084531492284], Loss = 0.23419009310561165\n",
      "Epoch 25: Predicted Output = [0.44352016270074396, 0.46039296752612446], Loss = 0.2342116701569321\n",
      "Epoch 26: Predicted Output = [0.4435236168379638, 0.46035548377536406], Loss = 0.23423301996154916\n",
      "Epoch 27: Predicted Output = [0.443527025757598, 0.4603183909226959], Loss = 0.23425414452847554\n",
      "Epoch 28: Predicted Output = [0.4435303903215599, 0.4602816858042354], Loss = 0.23427504586338344\n",
      "Epoch 29: Predicted Output = [0.44353371135539066, 0.46024536523607135], Loss = 0.2342957259677213\n",
      "Epoch 30: Predicted Output = [0.443536989650805, 0.4602094260178645], Loss = 0.2343161868379014\n",
      "Epoch 31: Predicted Output = [0.44354022596803533, 0.46017386493615936], Loss = 0.23433643046455113\n",
      "Epoch 32: Predicted Output = [0.44354342103799177, 0.46013867876743], Loss = 0.23435645883182504\n",
      "Epoch 33: Predicted Output = [0.4435465755642523, 0.46010386428088135], Loss = 0.23437627391677227\n",
      "Epoch 34: Predicted Output = [0.44354969022489593, 0.4600694182410255], Loss = 0.2343958776887542\n",
      "Epoch 35: Predicted Output = [0.44355276567419266, 0.46003533741005076], Loss = 0.23441527210891008\n",
      "Epoch 36: Predicted Output = [0.44355580254416194, 0.46000161854999844], Loss = 0.23443445912966682\n",
      "Epoch 37: Predicted Output = [0.4435588014460076, 0.45996825842476485], Loss = 0.23445344069428775\n",
      "Epoch 38: Predicted Output = [0.44356176297144356, 0.4599352538019397], Loss = 0.23447221873646015\n",
      "Epoch 39: Predicted Output = [0.4435646876939149, 0.4599026014544946], Loss = 0.23449079517991717\n",
      "Epoch 40: Predicted Output = [0.44356757616972514, 0.4598702981623345], Loss = 0.23450917193809126\n",
      "Epoch 41: Predicted Output = [0.44357042893907617, 0.4598383407137201], Loss = 0.234527350913798\n",
      "Epoch 42: Predicted Output = [0.4435732465270283, 0.4598067259065746], Loss = 0.23454533399894667\n",
      "Epoch 43: Predicted Output = [0.44357602944438757, 0.4597754505496806], Loss = 0.2345631230742773\n",
      "Epoch 44: Predicted Output = [0.44357877818852337, 0.4597445114637776], Loss = 0.2345807200091203\n",
      "Epoch 45: Predicted Output = [0.4435814932441261, 0.45971390548256663], Loss = 0.23459812666117918\n",
      "Epoch 46: Predicted Output = [0.44358417508390535, 0.45968362945363106], Loss = 0.23461534487633218\n",
      "Epoch 47: Predicted Output = [0.4435868241692376, 0.45965368023927705], Loss = 0.2346323764884542\n",
      "Epoch 48: Predicted Output = [0.4435894409507633, 0.45962405471730206], Loss = 0.23464922331925542\n",
      "Epoch 49: Predicted Output = [0.44359202586894075, 0.45959474978169695], Loss = 0.23466588717813638\n",
      "Epoch 50: Predicted Output = [0.44359457935455787, 0.45956576234328533], Loss = 0.23468236986205798\n",
      "Epoch 51: Predicted Output = [0.4435971018292076, 0.4595370893303052], Loss = 0.23469867315542636\n",
      "Epoch 52: Predicted Output = [0.44359959370572666, 0.4595087276889397], Loss = 0.23471479882998936\n",
      "Epoch 53: Predicted Output = [0.4436020553886037, 0.45948067438379625], Loss = 0.2347307486447467\n",
      "Epoch 54: Predicted Output = [0.4436044872743577, 0.4594529263983429], Loss = 0.23474652434587037\n",
      "Epoch 55: Predicted Output = [0.4436068897518884, 0.4594254807353011], Loss = 0.23476212766663632\n",
      "Epoch 56: Predicted Output = [0.443609263202803, 0.4593983344170009], Loss = 0.2347775603273652\n",
      "Epoch 57: Predicted Output = [0.44361160800171906, 0.45937148448570025], Loss = 0.23479282403537297\n",
      "Epoch 58: Predicted Output = [0.44361392451654585, 0.4593449280038702], Loss = 0.2348079204849292\n",
      "Epoch 59: Predicted Output = [0.44361621310874794, 0.45931866205445093], Loss = 0.23482285135722458\n",
      "Epoch 60: Predicted Output = [0.4436184741335883, 0.4592926837410782], Loss = 0.23483761832034428\n",
      "Epoch 61: Predicted Output = [0.44362070794035674, 0.45926699018828404], Loss = 0.2348522230292496\n",
      "Epoch 62: Predicted Output = [0.44362291487258276, 0.4592415785416733], Loss = 0.23486666712576498\n",
      "Epoch 63: Predicted Output = [0.4436250952682335, 0.4592164459680772], Loss = 0.23488095223857183\n",
      "Epoch 64: Predicted Output = [0.4436272494598998, 0.4591915896556857], Loss = 0.23489507998320802\n",
      "Epoch 65: Predicted Output = [0.44362937777496936, 0.4591670068141613], Loss = 0.23490905196207185\n",
      "Epoch 66: Predicted Output = [0.4436314805357898, 0.45914269467473445], Loss = 0.2349228697644316\n",
      "Epoch 67: Predicted Output = [0.4436335580598211, 0.4591186504902822], Loss = 0.23493653496643968\n",
      "Epoch 68: Predicted Output = [0.44363561065977775, 0.4590948715353909], Loss = 0.23495004913115075\n",
      "Epoch 69: Predicted Output = [0.4436376386437648, 0.459071355106406], Loss = 0.23496341380854416\n",
      "Epoch 70: Predicted Output = [0.44363964231540326, 0.45904809852146616], Loss = 0.23497663053555073\n",
      "Epoch 71: Predicted Output = [0.44364162197394913, 0.4590250991205278], Loss = 0.23498970083608137\n",
      "Epoch 72: Predicted Output = [0.4436435779144059, 0.45900235426537545], Loss = 0.23500262622106066\n",
      "Epoch 73: Predicted Output = [0.44364551042763123, 0.4589798613396232], Loss = 0.2350154081884633\n",
      "Epoch 74: Predicted Output = [0.4436474198004359, 0.4589576177487057], Loss = 0.23502804822335263\n",
      "Epoch 75: Predicted Output = [0.4436493063156795, 0.45893562091985873], Loss = 0.235040547797923\n",
      "Epoch 76: Predicted Output = [0.44365117025236045, 0.45891386830209396], Loss = 0.23505290837154366\n",
      "Epoch 77: Predicted Output = [0.44365301188570033, 0.45889235736616224], Loss = 0.23506513139080581\n",
      "Epoch 78: Predicted Output = [0.443654831487226, 0.45887108560451234], Loss = 0.2350772182895718\n",
      "Epoch 79: Predicted Output = [0.4436566293248463, 0.4588500505312413], Loss = 0.23508917048902603\n",
      "Epoch 80: Predicted Output = [0.4436584056629249, 0.4588292496820383], Loss = 0.23510098939772817\n",
      "Epoch 81: Predicted Output = [0.4436601607623514, 0.4588086806141231], Loss = 0.23511267641166853\n",
      "Epoch 82: Predicted Output = [0.44366189488060775, 0.4587883409061791], Loss = 0.23512423291432455\n",
      "Epoch 83: Predicted Output = [0.4436636082718322, 0.4587682281582799], Loss = 0.23513566027671923\n",
      "Epoch 84: Predicted Output = [0.44366530118688097, 0.45874833999181225], Loss = 0.23514695985748169\n",
      "Epoch 85: Predicted Output = [0.44366697387338644, 0.4587286740493949], Loss = 0.23515813300290728\n",
      "Epoch 86: Predicted Output = [0.4436686265758149, 0.45870922799479125], Loss = 0.2351691810470221\n",
      "Epoch 87: Predicted Output = [0.44367025953551914, 0.4586899995128202], Loss = 0.23518010531164577\n",
      "Epoch 88: Predicted Output = [0.44367187299079247, 0.45867098630926173], Loss = 0.2351909071064574\n",
      "Epoch 89: Predicted Output = [0.443673467176918, 0.4586521861107615], Loss = 0.23520158772906108\n",
      "Epoch 90: Predicted Output = [0.4436750423262177, 0.4586335966647282], Loss = 0.23521214846505473\n",
      "Epoch 91: Predicted Output = [0.4436765986680992, 0.4586152157392332], Loss = 0.23522259058809664\n",
      "Epoch 92: Predicted Output = [0.44367813642910203, 0.45859704112290245], Loss = 0.23523291535997654\n",
      "Epoch 93: Predicted Output = [0.44367965583294067, 0.45857907062481], Loss = 0.23524312403068426\n",
      "Epoch 94: Predicted Output = [0.4436811571005477, 0.45856130207436585], Loss = 0.23525321783848166\n",
      "Epoch 95: Predicted Output = [0.44368264045011535, 0.45854373332120446], Loss = 0.23526319800997353\n",
      "Epoch 96: Predicted Output = [0.4436841060971358, 0.4585263622350691], Loss = 0.23527306576018037\n",
      "Epoch 97: Predicted Output = [0.4436855542544401, 0.45850918670569524], Loss = 0.23528282229261127\n",
      "Epoch 98: Predicted Output = [0.4436869851322364, 0.4584922046426927], Loss = 0.23529246879933696\n",
      "Epoch 99: Predicted Output = [0.4436883989381471, 0.45847541397542496], Loss = 0.23530200646106464\n",
      "Epoch 100: Predicted Output = [0.4436897958772449, 0.4584588126528881], Loss = 0.23531143644721192\n"
     ]
    }
   ],
   "source": [
    "def forward_pass(inputs):\n",
    "    # Calculate the inputs to the hidden layer\n",
    "    h_input = [sum([inputs[j] * w[i][j] for j in range(len(inputs))]) + b[i] for i in range(len(b))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the hidden layer\n",
    "    h_output = [tanh(x) for x in h_input]\n",
    "\n",
    "    # Calculate the inputs to the output layer\n",
    "    o_input = [sum([h_output[j] * wh[i][j] for j in range(len(h_output))]) + bh[i] for i in range(len(bh))]\n",
    "\n",
    "    # Apply the sigmoid activation function to the output layer\n",
    "    o_output = [tanh(x) for x in o_input]\n",
    "\n",
    "    return h_output, o_output\n",
    "\n",
    "\n",
    "def backward_pass(inputs, h_output, o_output, target):\n",
    "    # Calculate the error in the output layer\n",
    "    o_error = [target[i] - o_output[i] for i in range(len(target))]\n",
    "\n",
    "    # Calculate the delta for the output layer\n",
    "    o_delta = [o_error[i] * tanh_derivative(o_output[i]) for i in range(len(o_error))]\n",
    "\n",
    "    # Calculate the error in the hidden layer\n",
    "    h_error = [sum([o_delta[j] * wh[j][i] for j in range(len(o_delta))]) for i in range(len(h_output))]\n",
    "\n",
    "    # Calculate the delta for the hidden layer\n",
    "    h_delta = [h_error[i] * tanh_derivative(h_output[i]) for i in range(len(h_error))]\n",
    "\n",
    "    return o_delta, h_delta\n",
    "    \n",
    "    \n",
    "def update_weights(inputs, h_output, o_delta, h_delta, lr):\n",
    "    global w, wh, b, bh, wo, bo\n",
    "\n",
    "    # Update weights and biases in the output layer\n",
    "    wo = [[wo[i][j] + lr * o_delta[i] * h_output[j] for j in range(len(wo[0]))] for i in range(len(wo))]\n",
    "    bo = [bo[i] + lr * o_delta[i] for i in range(len(bo))]\n",
    "\n",
    "    # Update weights and biases in the hidden layer\n",
    "    wh = [[wh[i][j] + lr * h_delta[i] * h_output[j] for j in range(len(wh[0]))] for i in range(len(wh))]\n",
    "    bh = [bh[i] + lr * h_delta[i] for i in range(len(bh))]\n",
    "\n",
    "    # Update weights and biases in the input layer\n",
    "    w = [[w[i][j] + lr * inputs[j] * h_delta[i] for j in range(len(w[0]))] for i in range(len(w))]\n",
    "    b = [b[i] + lr * h_delta[i] for i in range(len(b))]\n",
    "\n",
    "\n",
    "def train(inputs, target, lr):\n",
    "    h_output, o_output = forward_pass(inputs)\n",
    "    o_delta, h_delta = backward_pass(inputs, h_output, o_output, target)\n",
    "    update_weights(inputs, h_output, o_delta, h_delta, lr)\n",
    "    \n",
    "# Training inputs and target outputs\n",
    "inputs = [0.05, 0.10]\n",
    "target = [0.01, 0.99]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "epochs = 100\n",
    "\n",
    "for _ in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    \n",
    "    \n",
    "# Training Loop:\n",
    "for epoch in range(epochs):\n",
    "    train(inputs, target, lr)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        h_output, o_output = forward_pass(inputs)\n",
    "        print(f\"Epoch {epoch+1}: Predicted Output = {o_output}, Loss = {sum([(target[i] - o_output[i])**2 for i in range(len(target))])/2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
